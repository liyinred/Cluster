# Cluster_Wenhao
KNN（K-Nearest Neighbors）和K-Means是两种常用的机器学习算法，它们在原理和应用上有着本质的不同。

### KNN（K-最近邻）算法

**原理**：
- KNN是一种监督学习算法，用于分类和回归。
- 对于一个未知类别的样本，KNN算法会在训练集中找到与之最近的K个邻居，然后通过多数投票的方式来预测未知样本的类别（分类问题）或计算邻居的平均值（回归问题）。

**步骤**：
1. 确定邻居的数量K。
2. 计算未知样本与所有训练样本之间的距离。
3. 选择距离最近的K个样本。
4. 对分类问题，选择这K个样本中出现次数最多的类别作为预测结果；对回归问题，计算这K个样本的均值作为预测结果。

**特点**：
- KNN算法简单，不需要训练模型，计算量主要集中在预测阶段。
- K值的选择对结果影响较大。
- 对噪声敏感，需要预处理数据。

### K-Means（K-均值）算法

**原理**：
- K-Means是一种非监督学习算法，主要用于聚类。
- 算法接受一个参数K，然后将数据集中的样本划分为K个聚类，使得每个聚类内部样本之间的相似度尽可能高，而不同聚类之间的相似度尽可能低。

**步骤**：
1. 随机选择K个样本作为初始聚类中心。
2. 对数据集中的每一个样本，计算它与各个聚类中心的距离，并将其归到距离最近的聚类中心所在的类。
3. 根据聚类结果，重新计算每个聚类的中心点。
4. 重复步骤2和3，直到聚类中心不再发生变化或达到迭代次数上限。

**特点**：
- K值的选择对聚类结果影响很大，通常需要通过其他方法来确定K值。
- 初始聚类中心的选择会影响算法的收敛速度和最终结果。
- 对噪声和异常值比较敏感。

### 区别

- **学习类型**：KNN是监督学习，需要标注的训练数据；而K-Means是非监督学习，不需要标注的数据。
- **应用场景**：KNN用于分类和回归，K-Means仅用于聚类。
- **数据要求**：KNN在预测时计算量大，K-Means在训练时计算量大。
- **算法复杂度**：KNN简单直观，K-Means需要迭代寻找聚类中心。
